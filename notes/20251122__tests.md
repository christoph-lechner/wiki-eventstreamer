# Loading Tests

## Envisioned structure of the process:
* Downloader obtains fresh files via `rsync`
* Downloader registers every file in the DB table
* Downloader checks the first file (the files are sorted alphabetically, corresponding to chronological order with the current filename schema) for the timestamp of first event. This will be used as so-called "logical date" when submitting the run of the Loader DAG to Apache Airflow.
* Inside the DAG (filename `load_streamdump.py`) for *every file* in the filelist provided by the downloader
  * DAG ignores data files already known to loader (**TODO: implement filtering based on information in file table**)
  * DAG loads the file
  * DAG marks the file as processed, also stores statistical information (number of events in file, number of events that were merged into the events table)
  
## Working combination on 2025-Nov-22
### Commit IDs of files used
Running `file_transfer.py` (git commit id 70db271)
```
./file_transfer.py --register_in_db --trigger_DAG --trigger_DAG_override_timestamp
```
The script was executed under user `wikidata` (on `clpc` for the development)

Combined with loader DAGs from git commit id 8942563f

### Directory Structure
The modified configuration in `docker-compose.yaml` makes the local data directory
available inside of the containers under `/mnt`.

